# Project 3: Dataset Visualizer (`dataset_visualizer`)

## 1. Project Overview

This project is a hybrid system that combines an **Agentic Visualization Pipeline** with a **Custom MCP Server**.

1. **The Visualization Pipeline:** An intelligent multi-agent system that takes a raw CSV dataset (`tweets.csv`), analyzes its schema, plans the best way to visualize it, and writes the actual HTML/JavaScript code to render the chart. It features a "Self-Reflection" loop where it critiques and fixes its own code.
2. **The Context Server:** A custom MCP server (built with `FastMCP`) that exposes tools for searching Wikipedia and getting formatted time, likely intended to provide external context for the data analysis.

**Goal:** Automate the journey from "Raw Data" to "Interactive Dashboard" while providing external knowledge tools via MCP.

---

## 2. Architecture

### Part A: The Visualization Assembly Line (Scripts)

The system uses a chain of 6 specialized agents using `aisuite`:

| Agent | Role | Input | Output |
| --- | --- | --- | --- |
| **Agent 1** | **CSV Analyzer** | Raw Pandas DataFrame | JSON Schema (Data types, patterns) |
| **Agent 2** | **Decider** | Schema Analysis | Top 3 recommended plot types (e.g., Time-series, Bar chart) |
| **Agent 2.5** | **Planner** | Selected Plot | Step-by-step coding plan |
| **Agent 3** | **Coder** | Plan + Schema | `viz.html` (Raw HTML/JS code) |
| **Agent 4/5** | **Evaluator** | Raw Code | `viz3.html` (Refined, bug-fixed code) |
| **Agent 6** | **Grader** | Final Code | Rubric Score (0-130) |

### Part B: The MCP Server (`Twitter Server`)

This component runs alongside the pipeline to provide external tools.

* **Framework:** `FastMCP` (Python).
* **Tools:**
* `wikiSearch(query)`: Uses **LangChain's WikipediaRetriever** to find background info (e.g., "What event happened on this date in the tweets?").
* `get_date_time()`: Returns the current server time with specific formatting.



---

## 3. Technical Implementation Details

### The `output.js` Data Trick

Web browsers block local HTML files from reading local CSV files (`CORS error`). To bypass this without running a backend server, Agent 1 converts the Pandas DataFrame directly into a JavaScript variable:

```python
# Python side
with open('output.js', 'w') as f:
    f.write("const jsonData = " + df.to_json(orient='records') + ";\n")

```

The generated HTML then simply imports this file:

```html
<script src="output.js"></script>

```

### Self-Correction Loop

Agents 4 and 5 are **Reflective Agents**. They don't just write code; they read the code generated by Agent 3, check if it correctly references `jsonData`, ensure variable names match, and fix syntax errors before saving the final `viz3.html`.

### FastMCP Server

The server at the bottom of the code uses decorators to instantly turn Python functions into MCP tools.

```python
@mcp.tool()
def wikiSearch(query: str) -> str:
    # Connects to Wikipedia via LangChain
    return summary

```

This allows any MCP Client (like Claude Desktop or the clients from Project 1/2) to "plug in" to this script and start searching Wikipedia.

---

## 4. How to Run

### Prerequisites

* Install dependencies: `rich`, `pandas`, `aisuite`, `mcp`, `langchain-community`, `wikipedia`.
* Ensure you have a `tweets.csv` file in the directory.

### Step 1: Run the Visualization Pipeline

This will analyze the CSV and generate the HTML files.

```bash
python visualization_agent.py

```

* **Output:** You will see the agents "thinking" in the terminal (styled via `rich`).
* **Files Created:** `output.js`, `viz.html`, `viz2.html`, `viz3.html`.
* **View Result:** Open `viz3.html` in your web browser to see the interactive chart.

### Step 2: Run the MCP Server

To use the tools defined at the bottom, run the script as an MCP server.

```bash
# Run directly (stdio mode)
python twitter_server.py

```

*Note: This is usually run by an MCP Client (like in Project 1 or 2), not manually by a human.*

---

## 5. Why is this useful?

1. **Zero-Dependency Dashboards:** The agent builds a "Serverless" visualization. You can email the `viz3.html` and `output.js` to anyone, and it works without Python or Tableau.
2. **Context-Aware Analysis:** By combining the Visualization pipeline with the `wikiSearch` MCP tool, a future iteration of this agent could look at a spike in tweets on "Jan 6th," search Wikipedia for that date, and automatically annotate the chart with the real-world event.
